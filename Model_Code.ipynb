{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = dt.datetime.now()\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test_1 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据预处理,先删除一些没有奇异值\n",
    "train = train[train['passenger_count'] > 0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算时间特征,通过上述的分析我们发现时间特征是非常重要的一个维度，所以我们选择将其从数据中抽取出来\n",
    "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
    "test['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\n",
    "train['pickup_date'] = train['pickup_datetime'].dt.date\n",
    "test['pickup_date'] = test['pickup_datetime'].dt.date\n",
    "train['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\n",
    "\n",
    "train['pickup_weekday'] = train['pickup_datetime'].dt.weekday\n",
    "train['pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\n",
    "train['pickup_hour'] = train['pickup_datetime'].dt.hour\n",
    "train['pickup_minute'] = train['pickup_datetime'].dt.minute\n",
    "train['pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n",
    "train['pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n",
    "\n",
    "test['pickup_weekday'] = test['pickup_datetime'].dt.weekday\n",
    "test['pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\n",
    "test['pickup_hour'] = test['pickup_datetime'].dt.hour\n",
    "test['pickup_minute'] = test['pickup_datetime'].dt.minute\n",
    "test['pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\n",
    "test['pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']\n",
    "\n",
    "train['pickup_dayofyear'] = train['pickup_datetime'].dt.dayofyear\n",
    "test['pickup_dayofyear'] = test['pickup_datetime'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算出发点到目的地的角度方向,参考的是wiki的内容\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6378.137  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "train['direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n",
    "                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n",
    "\n",
    "test['direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n",
    "                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 下面的这些特征的提取来源于kaggle论坛的讨论,因为大家认为地图上的欧几里得距离并不能真实反映实际的情况，所以应该\n",
    "# 加入街道距离等.\n",
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def Dummy_MahaDis(lat1, lng1, lat2, lng2):\n",
    "    tmp1 = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    tmp2 = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return tmp1 + tmp2\n",
    "\n",
    "train['distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n",
    "train['distance_dummy_manhattan'] = Dummy_MahaDis(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n",
    "\n",
    "\n",
    "test['distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n",
    "test['distance_dummy_manhattan'] = Dummy_MahaDis(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\n",
    "\n",
    "# 计算中间的经纬度，参考notebook:https://www.kaggle.com/mubashir44/xgboost-with-kfold-valid-lb-0-368\n",
    "train['center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\n",
    "train['center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\n",
    "test['center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\n",
    "test['center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算pca的距离，参考notebook:https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367,因为真实的维度就只有4维,\n",
    "# 所以我们这边选用其中的三维.\n",
    "coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n",
    "                    test[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "\n",
    "train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1] \n",
    "train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1] \n",
    "test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1] \n",
    "test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算pca的曼哈顿距离,参考的notebook：notebook:https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367\n",
    "train.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n",
    "test.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 对store_and_fwd_flag进行编码，主要是为了方便我们的model的运行\n",
    "train['store_and_fwd_flag'] = train['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)\n",
    "test['store_and_fwd_flag'] = test['store_and_fwd_flag'].map(lambda x: 0 if x == 'N' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 每次采样300000个样本,然后进行K-means的聚类操作,这边的30000可以自己进行调试.\n",
    "indexes = np.random.permutation(len(coords))[:300000]\n",
    "kmeans = MiniBatchKMeans(n_clusters=50, batch_size=10000).fit(coords[indexes])\n",
    "train.loc[:, 'pickup_cluster'] = kmeans.predict(train[['pickup_latitude', 'pickup_longitude']])\n",
    "train.loc[:, 'dropoff_cluster'] = kmeans.predict(train[['dropoff_latitude', 'dropoff_longitude']])\n",
    "test.loc[:, 'pickup_cluster'] = kmeans.predict(test[['pickup_latitude', 'pickup_longitude']])\n",
    "test.loc[:, 'dropoff_cluster'] = kmeans.predict(test[['dropoff_latitude', 'dropoff_longitude']])\n",
    "t1 = dt.datetime.now() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这两个数据是后来其他人分享的,所以我们直接将其加入到我们的模型中来\n",
    "train_part_1 = pd.read_csv('./data/fastest_routes_train_part_1.csv', usecols=['id', 'total_distance', 'total_travel_time',  'number_of_steps', ])\n",
    "train_part_2 = pd.read_csv('./data/fastest_routes_train_part_2.csv', usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n",
    "test_street_info = pd.read_csv('./data/fastest_routes_test.csv',usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\n",
    "train_street_info = pd.concat((train_part_1, train_part_2))\n",
    "train = train.merge(train_street_info, how='left', on='id')\n",
    "test = test.merge(test_street_info, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 为了方便我们的指标的计算,所以此处我们将其换算成log的形式\n",
    "train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备训练和测试数据\n",
    "feature_names = list(train.columns) \n",
    "not_use_names = ['id', 'log_trip_duration', 'trip_duration', 'dropoff_datetime', 'pickup_date', 'pickup_datetime', 'date']\n",
    "feature_names = [x for x in train.columns if f not in not_use_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgboost训练得到模型1\n",
    "train_X = train[feature_names].values\n",
    "train_y = np.log(train['trip_duration'].values + 1)  \n",
    " \n",
    "test_X = test[feature_names].values\n",
    "dtrain = xgb.DMatrix(train_X, label=train_y) \n",
    "dtest = xgb.DMatrix(test_X) \n",
    "xgb_pars = { 'booster' : 'gbtree','min_child_weight': 12, 'eta': 0.05, 'max_depth': 10, 'colsample_bytree': 0.9,\n",
    "            'subsample': 0.9, 'lambda': 5, 'nthread': -1, 'silent': 1, 'eval_metric': 'rmse', 'objective': 'reg:linear'}    \n",
    "\n",
    "model = xgb.train(xgb_pars, dtrain, 500, maximize=False, verbose_eval=15)\n",
    "res_xgb = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# catboost训练得到模型2\n",
    "cat = cbt.CatBoostRegressor(iterations= 8000, learning_rate= 0.1, depth = 6, random_seed=1)\n",
    "cat.fit(train_X,train_y) \n",
    "res_cat = cat.predict(test_X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lightGBM训练得到模型3\n",
    "lgb_train = lgb.Dataset(train_X, train_y)\n",
    " \n",
    "params = {'metric': 'rmse', 'learning_rate' : 0.05, 'num_leaves': 311, \n",
    "         'feature_fraction': 0.9,'bagging_fraction':0.9,'bagging_freq':5,'min_data_in_leaf': 500}\n",
    "lgb_model = lgb.train(params, lgb_train, num_boost_round = 350)\n",
    "res_lgb = lgb_model.predict(lgb.Dataset(test_X.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型加权集成\n",
    "final_res = 0.45 * res_xgb + 0.3 * res_cat + 0.25 * res_lgb\n",
    "\n",
    "test['trip_duration'] = np.exp(final_res) - 1\n",
    "test.loc[test['trip_duration']<0,'trip_duration'] = 0\n",
    "test[['id', 'trip_duration']].to_csv('./predict/submit.csv', index=False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
